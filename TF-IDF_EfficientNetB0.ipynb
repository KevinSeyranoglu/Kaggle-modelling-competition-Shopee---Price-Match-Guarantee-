{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n#reading data\nimport pickle\n\n#Data visutalisation \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n#Tensorflow, teras \nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, Concatenate , Input\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, MaxPool2D,AveragePooling2D,MaxPooling2D,BatchNormalization\nfrom sklearn.metrics import  accuracy_score as score\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\n\n# for Texts \nimport sklearn.feature_extraction.text as text\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\nimport matplotlib.pyplot as plt \nfrom sklearn import preprocessing\nfrom nltk.stem import WordNetLemmatizer \nimport re\n\n#dim reduction\nfrom sklearn.decomposition import PCA\nfrom tqdm.notebook import tqdm as tqdm\n\nGPU=False\n\ndef cosine_sim(X2,batch_size,N_neighbor):\n    #X2=cupy.asarray(X)\n    df_cos_dist=pd.DataFrame(columns=np.arange(N_neighbor))\n    df_id=pd.DataFrame(columns=np.arange(N_neighbor))\n    X2_T=X2.T\n    a=0\n    b=batch_size\n    n_it=int(len(df)/batch_size)\n\n    for i in tqdm(range(n_it)):\n        df_cos_dist_batch=(1-X2[a:b]@X2_T)\n\n        args=np.argsort(df_cos_dist_batch)[:,0:N_neighbor]\n        df_cos_dist_batch=np.sort(df_cos_dist_batch)[:,0:N_neighbor]\n        \n\n        df_dist=pd.DataFrame(df_cos_dist_batch)\n        arg_df=pd.DataFrame(args)\n\n        df_cos_dist = pd.concat([df_cos_dist,df_dist],ignore_index=True)\n        df_id = pd.concat([df_id,arg_df],ignore_index=True)\n\n        a+=batch_size\n        b+=batch_size\n\n    df_cos_dist_batch=(1-X2[a:]@X2_T)\n    args=np.argsort(df_cos_dist_batch)[:,0:N_neighbor]\n    df_cos_dist_batch=np.sort(df_cos_dist_batch)[:,0:N_neighbor]\n\n\n    df_dist=pd.DataFrame(df_cos_dist_batch)\n    arg_df=pd.DataFrame(args)\n\n    df_cos_dist = pd.concat([df_cos_dist,df_dist],ignore_index=True)\n    df_id = pd.concat([df_id,arg_df],ignore_index=True)\n    return np.array(df_cos_dist),np.array(df_id)\n\n\nif GPU:\n    import cupy \n    def cosine_sim(X,batch_size,N_neighbor):\n        X2=cupy.asarray(X)\n        df_cos_dist=pd.DataFrame(columns=np.arange(N_neighbor))\n        df_id=pd.DataFrame(columns=np.arange(N_neighbor))\n\n        a=0\n        b=batch_size\n        n_it=int(len(df)/batch_size)\n\n        for i in tqdm(range(n_it)):\n            df_cos_dist_batch=(1-cupy.matmul(X2,X2[a:b].T).T)\n\n            args=cupy.argsort(df_cos_dist_batch)[:,0:N_neighbor]\n            df_cos_dist_batch=cupy.sort(df_cos_dist_batch)[:,0:N_neighbor]\n\n            df_cos_dist_batch=cupy.asnumpy(df_cos_dist_batch)\n            args=cupy.asnumpy(args)\n\n            df_dist=pd.DataFrame(df_cos_dist_batch)\n            arg_df=pd.DataFrame(args)\n\n            df_cos_dist = pd.concat([df_cos_dist,df_dist],ignore_index=True)\n            df_id = pd.concat([df_id,arg_df],ignore_index=True)\n\n            a+=batch_size\n            b+=batch_size\n\n        df_cos_dist_batch=(1-cupy.matmul(X2,X2[a:].T).T)\n        args=cupy.argsort(df_cos_dist_batch)[:,0:N_neighbor]\n        df_cos_dist_batch=cupy.sort(df_cos_dist_batch)[:,0:N_neighbor]\n\n        df_cos_dist_batch=cupy.asnumpy(df_cos_dist_batch)\n        args=cupy.asnumpy(args)\n\n        df_dist=pd.DataFrame(df_cos_dist_batch)\n        arg_df=pd.DataFrame(args)\n\n        df_cos_dist = pd.concat([df_cos_dist,df_dist],ignore_index=True)\n        df_id = pd.concat([df_id,arg_df],ignore_index=True)\n        return np.array(df_cos_dist),np.array(df_id)\n\ndf = pd.read_csv('../input/shopee-product-matching/train.csv')\n\n\n\n\n\n\n\n\ndef th_len_df(len_of_df):\n    return -1.768e-06*len_of_df+0.6269\n\n\n\n\ntrain=True\nif train==True:\n    df = pd.read_csv('../input/shopee-product-matching/train.csv')\nif train==False:\n    df = pd.read_csv('../input/shopee-product-matching/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"if train:\n    N_g_sample=False \n    if N_g_sample:\n\n        N_g=100\n        unique_label_group=df.label_group.unique()\n        inds=np.arange(len(unique_label_group))\n        np.random.seed(seed=50)\n        np.random.shuffle(inds)\n\n        inds=inds[0:N_g]\n\n        unique_label_group=unique_label_group[inds]\n\n\n        L=[]\n        for i in range(N_g):\n            L.extend(list(df.loc[df.label_group==unique_label_group[i]].index.values))\n\n        np.random.shuffle(L)\n\n\n        df=df.iloc[L]\n        df=df.reset_index(drop=True)\n\n","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"if train:\n    a=df.groupby('label_group').count()['image'].mean()\n    print(a)\n#avg=3.109678590884329\nN_groups=4096","metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"3.109678590884329\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cleaning the data  for the titles \n\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\n# to remove the stop words\nstop_words = stopwords.words(\"english\")\n\n\nstemmer = SnowballStemmer(\"english\")\nlemmatizer = WordNetLemmatizer() \n\n#preprocess fontion \ndef preprocess(text, stem=False,lem=True):\n    # Remove link and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            if lem:\n                tokens.append(lemmatizer.lemmatize(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)\n\n# cleaning the data for every rows\ndf.title= df.title.apply(lambda x: preprocess(x))\n\n# TF-Idf\n#---------------------------------------------------\n#tfidf = text.TfidfVectorizer(max_features=5000) #ok \n#tfidf = text.TfidfVectorizer(max_features=10000) #ok \ntfidf = text.TfidfVectorizer(min_df=2) #ok \n\n\ntfidf.fit(df.title)\nt = tfidf.transform(df.title)\n#just making a matrix \nmatrice=pd.DataFrame(t.todense())\nmatrice.columns=tfidf.get_feature_names()\n\nt=(t.todense())\n#---------------------------------------------------","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.preprocessing import image\nfrom keras.applications.imagenet_utils import decode_predictions, preprocess_input\nfrom keras.models import Model\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom keras.layers import Conv2D, MaxPooling2D, InputLayer, Flatten,    Dense, BatchNormalization, AveragePooling2D\nfrom keras.models import Sequential\n\n\n\n\nWGT='../input/effnet/efficientnetb0.h5'\n\npretrained_model = EfficientNetB0(weights=WGT, include_top=True)\nfeat_extractor = Model(inputs=pretrained_model.input, outputs=pretrained_model.get_layer(\"top_dropout\").output)\n\n#shape of the loaded image for the pretrained model \nim_input_shape=feat_extractor.output.shape[1:]\n\n#the shape is the number of columns of he TF-IDF matrix\ntfidf_input_shape=t.shape[1]","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"Im_input_shape=list(pretrained_model.input.shape)[1:]\nprint(Im_input_shape)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[224, 224, 3]\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, Concatenate\n\n\nx_im_feat = Input(shape=im_input_shape, name=\"Images-data\")\ntfifd_input = Input(shape=tfidf_input_shape, name=\"tf-idf-data\")\n\n\nx = Dense(N_groups , activation='relu')(x_im_feat)\n\nx_Tf_idf = Dense(N_groups , activation='relu')(tfifd_input)\n\nx=x*x_Tf_idf\n\nmodel = Model(inputs=[x_im_feat,tfifd_input], outputs=x, name='test')\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model: \"test\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nImages-data (InputLayer)        [(None, 1280)]       0                                            \n__________________________________________________________________________________________________\ntf-idf-data (InputLayer)        [(None, 14922)]      0                                            \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 4096)         5246976     Images-data[0][0]                \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 4096)         61124608    tf-idf-data[0][0]                \n__________________________________________________________________________________________________\ntf.math.multiply_1 (TFOpLambda) (None, 4096)         0           dense_2[0][0]                    \n                                                                 dense_3[0][0]                    \n==================================================================================================\nTotal params: 66,371,584\nTrainable params: 66,371,584\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"DATADIR=\"../input/shopee-product-matching/\"\n\n#new columns that has the dir + the name of the image\nif train:\n    df['Im_dir'] = DATADIR + 'train_images/' + df['image']\nif train==False:\n    df['Im_dir'] = DATADIR + 'test_images/' + df['image']\n    \nbatch_size=500\nn_it=int(len(df)/batch_size)\na=0\nb=batch_size\nbatch_list=[]\ndf2=pd.DataFrame(columns=np.arange(model.output.shape[1]))\n\nfor i in range(n_it):\n    batch_list.append(df[a:b])\n    a+=batch_size\n    b+=batch_size\nbatch_list.append(df[a:])\n\nfor i in tqdm(range(len(batch_list))):\n    df_batch=batch_list[i]\n    #the size of the image\n\n    #Empty list \n    data_im=[]\n\n    IM_SIZE=Im_input_shape[0]\n    COLOR_MODE='rgb' \n    #the directory for all individual images\n    images=df_batch.Im_dir.values\n\n    #loop over batch of the images \n    for im in (images):\n\n        #reshaping the data into a IM_SIZE X IM_SIZE image\n        img_array=tf.keras.preprocessing.image.load_img(im,target_size=(IM_SIZE,IM_SIZE),color_mode=COLOR_MODE)\n        img_array = tf.keras.preprocessing.image.img_to_array(img_array)\n        \n        #Normalization\n        #normalizing the image gives us a worste result \n        #img_array=img_array/255.0 \n        \n        data_im.append(img_array)\n    data_im_arr=np.array(data_im)\n\n    ###################################################################\n    pretrained_arr=feat_extractor.predict(data_im_arr)\n\n    T=t[batch_list[i].index]\n    \n    pred_n=model.predict([pretrained_arr,T])\n    \n    df1=pd.DataFrame(pred_n)\n    \n    df2 = pd.concat([df2,df1],ignore_index=True)\n","metadata":{"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/69 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694f6777879e4a6ebe679011c437bd61"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import Normalizer\ntransformer = Normalizer(norm='l2').fit(df2)  # fit does nothing.\n\ntransformer\nif len(df)>3:\n    N_neighbor=50\n    N_bach=500\n    \nif len(df)<50:\n    N_neighbor=1\n    N_bach=1\n    \nX=transformer.transform(df2)\nKnn=cosine_sim(X,N_bach,N_neighbor)\n\ndistances=Knn[0]\nindices=Knn[1]","metadata":{"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/68 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d343e3702f4aeba7adcd5fd205044d"}},"metadata":{}}]},{"cell_type":"code","source":"if train:\n    n=99\n    display(Knn[0][n])\n    display(Knn[1][n])\n    pred=Knn[1][n]\n\n\n    dn=df.loc[df.label_group==df.iloc[n].label_group]\n    arr=dn.index\n    print('N=',len(arr))\n    for p in pred:\n        print(p in arr)\n\n","metadata":{"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"array([5.3644180e-07, 4.0718979e-01, 5.0870097e-01, 5.2349746e-01,\n       5.4362530e-01, 5.4950738e-01, 5.5635583e-01, 5.5961215e-01,\n       5.6538320e-01, 5.8138454e-01, 5.9051549e-01, 5.9678829e-01,\n       6.0401273e-01, 6.0689139e-01, 6.0903347e-01, 6.1490273e-01,\n       6.1807609e-01, 6.2834013e-01, 6.2938416e-01, 6.6281438e-01,\n       6.6284931e-01, 6.6861010e-01, 6.7649293e-01, 6.7675841e-01,\n       6.7942882e-01, 6.8585086e-01, 6.8633306e-01, 6.9005597e-01,\n       6.9168711e-01, 6.9557828e-01, 7.0170110e-01, 7.0204604e-01,\n       7.0356756e-01, 7.0965964e-01, 7.1188807e-01, 7.1228564e-01,\n       7.1514010e-01, 7.1902949e-01, 7.2242498e-01, 7.2373176e-01,\n       7.2435617e-01, 7.3952162e-01, 7.4167979e-01, 7.4334234e-01,\n       7.4428868e-01, 7.4448955e-01, 7.4508679e-01, 7.4977744e-01,\n       7.5000143e-01, 7.5015813e-01], dtype=float32)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([99, 27075, 33322, 8668, 26483, 6734, 63, 75, 98, 12393, 2608,\n       13476, 21204, 14491, 24157, 31721, 17811, 15767, 7526, 11586, 2517,\n       32578, 12932, 22170, 33423, 6201, 28282, 2457, 1186, 32423, 12706,\n       11757, 28489, 16143, 21094, 27146, 18326, 14544, 2289, 30531,\n       12688, 13259, 21873, 13473, 82, 10256, 28909, 3168, 30319, 4300],\n      dtype=object)"},"metadata":{}},{"name":"stdout","text":"N= 41\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n","output_type":"stream"}]},{"cell_type":"code","source":"if train:\n    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n    df['matches'] = df['label_group'].map(tmp)\n    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"if train:\n    # Function to get our f1 score\n    def f1_score(y_true, y_pred):\n        y_true = y_true.apply(lambda x: set(x.split()))\n        y_pred = y_pred.apply(lambda x: set(x.split()))\n        intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n        len_y_pred = y_pred.apply(lambda x: len(x)).values\n        len_y_true = y_true.apply(lambda x: len(x)).values\n        f1 = 2 * intersection / (len_y_pred + len_y_true)\n        return f1","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"if train:\n    predictions = []\n    threshold=th_len_df(len(df))\n\n\n    for k in range(df2.shape[0]):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n        predictions.append(posting_ids)\n    df['pred_matches'] = predictions\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(score)","metadata":{"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"0.7313481664692026\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## mean F1 score of 0.731 with train Data ","metadata":{}},{"cell_type":"code","source":"if train:\n    #optimizing the threshold\n    thresholds=np.arange(0.56,0.60,0.001)\n    s=[]\n    for th in tqdm(thresholds):\n        predictions = []\n\n        indices=Knn[1]\n        for k in range(df2.shape[0]):\n            idx = np.where(distances[k,] < th)[0]\n            ids = indices[k,idx]\n            posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n            predictions.append(posting_ids)\n        df['pred_matches'] = predictions\n        df['f1'] = f1_score(df['matches'], df['pred_matches'])\n        score = df['f1'].mean()\n        s.append(score)\n    s=np.array(s)\n    print(thresholds[np.argmax(s)])","metadata":{"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8602960efbc3422fa335666c0e549577"}},"metadata":{}},{"name":"stdout","text":"0.561\n","output_type":"stream"}]},{"cell_type":"code","source":"th_len_df(len(df))","metadata":{"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"0.566346"},"metadata":{}}]},{"cell_type":"code","source":"predictions = []\nthreshold=th_len_df(len(df))\n\nindices=Knn[1]\ndistances=Knn[0]\nfor k in range(df2.shape[0]):\n    idx = np.where(distances[k,] < threshold)[0]\n    ids = indices[k,idx]\n    posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n    predictions.append(posting_ids)\ndf['matches'] = predictions","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"         posting_id                                            matches\n0   train_129225211                   train_129225211 train_2278313361\n1  train_3386243561  train_3386243561 train_3423213080 train_380550...\n2  train_2288590299                  train_2288590299 train_3803689425\n3  train_2406599165  train_2406599165 train_3576714541 train_352677...\n4  train_3369186413                   train_3369186413 train_921438619","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>matches</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>train_129225211 train_2278313361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>train_3386243561 train_3423213080 train_380550...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>train_2288590299 train_3803689425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>train_2406599165 train_3576714541 train_352677...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>train_3369186413 train_921438619</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}